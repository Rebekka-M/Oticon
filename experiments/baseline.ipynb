{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# Models\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score\n",
    "\n",
    "# PyTorch\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Data Loading \n",
    "os.chdir(\"..\")\n",
    "from data.loader import load_training, load_validation, load_testing, n_freq, n_time, n_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: If you are training/experimenting, use the training and validation set instead.\n",
      "Use this function only for the very final evaluation before the competition ends.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((33849, 32, 96), (8463, 32, 96), (10578, 32, 96))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = load_training()\n",
    "X_val, y_val = load_validation()\n",
    "X_test, y_test = load_testing()\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy model (baseline): Predicting the most frequent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: \t 3\n",
      "\n",
      "Testing accuracy: \t0.5169219134051806, \tF1: 0.1363081141717562, \tlog loss: 17.41189911310159\n"
     ]
    }
   ],
   "source": [
    "clf_dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "clf_dummy.fit(X_train, y_train)\n",
    "\n",
    "# Predict on Validation Set and Testing set\n",
    "y_pred_val = clf_dummy.predict(X_val)\n",
    "y_pred_test = clf_dummy.predict(X_test)\n",
    "\n",
    "print(f\"Number of parameters: \\t {len(clf_dummy.get_params())}\")\n",
    "print()\n",
    "print(f\"Testing accuracy: \\t{accuracy_score(y_test, y_pred_test)}, \\tF1: {f1_score(y_test, y_pred_test, average = 'macro')}, \\tlog loss: {log_loss(y_test, clf_dummy.predict_proba(X_test))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rebek\\miniconda3\\envs\\oticon\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: \t 6\n",
      "\n",
      "Validation accuracy: \t0.6256646579227224, \tF1: 0.3352538981891634, \tlog loss: 0.9949750429151191\n",
      "Testing accuracy: \t0.6179807146908678, \tF1: 0.3196391238192207, \tlog loss: 0.9981086430667209\n"
     ]
    }
   ],
   "source": [
    "clf_lreg = LogisticRegression(\n",
    "    multi_class=\"multinomial\",\n",
    ")\n",
    "clf_lreg.fit(X_train.reshape(-1, n_freq * n_time), y_train)\n",
    "\n",
    "# Predict on testing set\n",
    "y_pred_test = clf_lreg.predict(X_test.reshape(-1, n_freq * n_time))\n",
    "\n",
    "print(f\"Number of parameters: \\t {32 * 96 * 5 + 1}\")\n",
    "print()\n",
    "print(f\"Testing accuracy: \\t{accuracy_score(y_test, y_pred_test)}, \\tF1: {f1_score(y_test, y_pred_test, average = 'macro')}, \\tlog loss: {log_loss(y_test, clf_lreg.predict_proba(X_test.reshape(-1, n_freq * n_time)))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (Multi-layer Perceptron): Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: \t 410501\n",
      "\n",
      "Testing accuracy: \t0.6929476271506901, \tF1: 0.47741689749917027, \tlog loss: 0.8317131750195447\n"
     ]
    }
   ],
   "source": [
    "# Lets use a simple neural network as a baseline\n",
    "clf_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 128),\n",
    "    max_iter=1000,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.2,\n",
    "    random_state=133742069,\n",
    ")\n",
    "\n",
    "clf_mlp.fit(X_train.reshape(-1, n_freq * n_time), y_train)\n",
    "\n",
    "y_pred_test = clf_mlp.predict(X_test.reshape(-1, n_freq * n_time))\n",
    "\n",
    "print(f\"Number of parameters: \\t {sum([a.size for a in clf_mlp.coefs_]) +  sum([a.size for a in clf_mlp.intercepts_])}\")\n",
    "print()\n",
    "print(f\"Testing accuracy: \\t{accuracy_score(y_test, y_pred_test)}, \\tF1: {f1_score(y_test, y_pred_test, average = 'macro')}, \\tlog loss: {log_loss(y_test, clf_mlp.predict_proba(X_test.reshape(-1, n_freq * n_time)))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet-alike architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_cpu = False\n",
    "\n",
    "if t.cuda.is_available() and not force_cpu:\n",
    "    device = t.device(\"cuda\")\n",
    "else:\n",
    "    device = t.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "X_map = lambda X: t.from_numpy(X).to(dtype=t.float)\n",
    "y_map = lambda y: t.from_numpy(y).to(dtype=t.uint8)\n",
    "loader_map = lambda data: DataLoader(\n",
    "    dataset=data,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=\"cuda\" == device,\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test = map(X_map, (X_train, X_val, X_test))\n",
    "y_train, y_val, y_test = map(y_map, (y_train, y_val, y_test))\n",
    "\n",
    "data_train = TensorDataset(X_train, y_train)\n",
    "data_val = TensorDataset(X_val, y_val)\n",
    "data_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "loader_train, loader_val, loader_test = map(loader_map, (data_train, data_val, data_test))\n",
    "\n",
    "n_train = len(X_train)\n",
    "n_val = len(X_val)\n",
    "n_test = len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 12793\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b5fec50abf4bee8228323a0b470d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.Conv1d(32, 16, 3),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool1d(3),\n",
    "            nn.Conv1d(16, 4, 1),\n",
    "            nn.Tanh(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(120, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, X: t.Tensor) -> t.Tensor:\n",
    "        return self.model(X)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "n_epochs = 20\n",
    "\n",
    "model = Model().to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "acc_running = 0\n",
    "\n",
    "for epoch in (bar:=trange(n_epochs)):\n",
    "    for x, y in loader_train:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        acc = (y_pred.argmax(dim=1) == y).sum() / y.size(0)\n",
    "        acc_running += 0.05 * (acc.item() - acc_running)\n",
    "        bar.set_postfix(acc=f\"{acc_running:.2f}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: \t 12793\n",
      "\n",
      "Testing accuracy: \t0.4155795046322556, \tF1: 0.18802420876187903, \tlog loss: 6.588488782856262\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "y_pred = t.zeros(n_test, n_classes, device=device)\n",
    "\n",
    "for i, (x, y) in enumerate(loader_test):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    y_pred[i*128: 128 + i*128,:] = model(x)\n",
    "\n",
    "print(f\"Number of parameters: \\t {sum(p.numel() for p in model.parameters())}\")\n",
    "print()\n",
    "print(f\"Testing accuracy: \\t{accuracy_score(y_test, y_pred.argmax(dim=1).detach().numpy())}, \\tF1: {f1_score(y_test, y_pred.argmax(dim=1).detach().numpy(), average = 'macro')}, \\tlog loss: {log_loss(y_test, y_pred.detach().numpy())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('oticon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d125b846dca7da9364c6220315daf374b7a8270fbd1aafae7808a5a0b568fce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
